{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "073f539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9425682",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "493efff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_index</th>\n",
       "      <th>superclass_index</th>\n",
       "      <th>subclass_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6467</th>\n",
       "      <td>6467.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6468</th>\n",
       "      <td>6468.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6469</th>\n",
       "      <td>6469.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6470</th>\n",
       "      <td>6470.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6471</th>\n",
       "      <td>6471.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6472 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_index  superclass_index  subclass_index\n",
       "0          0.jpg                 1              53\n",
       "1          1.jpg                 1              45\n",
       "2          2.jpg                 1               4\n",
       "3          3.jpg                 2               7\n",
       "4          4.jpg                 1              33\n",
       "...          ...               ...             ...\n",
       "6467    6467.jpg                 0              55\n",
       "6468    6468.jpg                 0              84\n",
       "6469    6469.jpg                 0              49\n",
       "6470    6470.jpg                 2              71\n",
       "6471    6471.jpg                 0              15\n",
       "\n",
       "[6472 rows x 3 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('train_label.csv')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fa7805fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subclass labels of each super class\n",
    "class0 = labels[labels['superclass_index']==0]\n",
    "class1 = labels[labels['superclass_index']==1]\n",
    "class2 = labels[labels['superclass_index']==2]\n",
    "sub_class0 = class0.subclass_index.unique()\n",
    "sub_class1 = class1.subclass_index.unique()\n",
    "sub_class2 = class2.subclass_index.unique()\n",
    "#labels.subclass_index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8ca4ffd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\nimport shutil\\ntrain_path = 'train_shuffle/'\\ncleaned_ds = 'cleaned_train_dataset/'\\nfor i in range(len(labels)):\\n    image_path = os.path.join(train_path, labels['image_index'][i])\\n    new_path = cleaned_ds+str(labels['superclass_index'][i])+'/'+str(labels['subclass_index'][i])+'/'\\n    new_image = cleaned_ds+str(labels['superclass_index'][i])+'/'+str(labels['subclass_index'][i])+'/'+labels['image_index'][i]\\n    if os.path.exists(new_path) == True:\\n        shutil.copyfile(image_path, new_image)\\n    else:\\n        os.mkdir(new_path)\\n        shutil.copyfile(image_path, new_image)\\n\""
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a image folder which contain labels\n",
    "'''\n",
    "import os\n",
    "import shutil\n",
    "train_path = 'train_shuffle/'\n",
    "cleaned_ds = 'cleaned_train_dataset/'\n",
    "for i in range(len(labels)):\n",
    "    image_path = os.path.join(train_path, labels['image_index'][i])\n",
    "    new_path = cleaned_ds+str(labels['superclass_index'][i])+'/'+str(labels['subclass_index'][i])+'/'\n",
    "    new_image = cleaned_ds+str(labels['superclass_index'][i])+'/'+str(labels['subclass_index'][i])+'/'+labels['image_index'][i]\n",
    "    if os.path.exists(new_path) == True:\n",
    "        shutil.copyfile(image_path, new_image)\n",
    "    else:\n",
    "        os.mkdir(new_path)\n",
    "        shutil.copyfile(image_path, new_image)\n",
    "'''\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff07ba",
   "metadata": {},
   "source": [
    "### calculate mean and std of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "441be1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images_count = 0\n",
    "    for images, _ in loader:\n",
    "        images = images.view(images.size(0), images.size(1),-1)\n",
    "        mean +=images.mean(2).sum(0)\n",
    "        std +=images.std(2).sum(0)\n",
    "        total_images_count += images.size(0)\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    return mean, std\n",
    "training_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.ImageFolder(root='super-sub-data/super-sub', \n",
    "                                                 transform = training_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c3b156c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6136, 0.6157, 0.6193])\n"
     ]
    }
   ],
   "source": [
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4594e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    if torch.cuda.is_available():\n",
    "        dev = 'cuda:0'\n",
    "    else:\n",
    "        dev = 'cpu'\n",
    "    return torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8fd48571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 subfolders in the folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder_path = 'super-sub-data/super-sub/'\n",
    "folder_contents = os.listdir(folder_path)\n",
    "\n",
    "# Count the number of subfolders in the folder\n",
    "num_subfolders = sum([os.path.isdir(os.path.join(folder_path, item)) for item in folder_contents])\n",
    "\n",
    "# Print the result\n",
    "print(f'There are {num_subfolders} subfolders in the folder.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ea810",
   "metadata": {},
   "source": [
    "### transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "099a9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "test_path = 'test_shuffle/'\n",
    "train_path = 'super-sub-data/super-sub/'\n",
    "####################tbd\n",
    "mean, std = get_mean_std(train_loader)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=(0, 180)),\n",
    "    transforms.RandomAutocontrast(p=0.5),\n",
    "    #transforms.RandomAdjustSharpness(sharpness_factor=2,p=0.5),\n",
    "    transforms.ColorJitter()\n",
    "    #transforms.RandomEqualize()\n",
    "    #transforms.AugMix(),\n",
    "    #transforms.ToTensor(),\n",
    "    #transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))\n",
    "    \n",
    "])\n",
    "#randaugment_transforms = transforms.Compose([transforms.RandAugment()])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    #transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))\n",
    "    \n",
    "])\n",
    "class DatasetFromSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "dataset = torchvision.datasets.ImageFolder(root = train_path, transform = test_transforms)\n",
    "#build my own class\n",
    "#\n",
    "\n",
    "\n",
    "#dataset = torch.utils.data.DataLoader(dataset=dataset, shuffle=True)\n",
    "\n",
    "# n = len(dataset)  # total number of examples\n",
    "# n_test = int(0.13 * n)  # take ~15% for test\n",
    "# test_temp_set = torch.utils.data.Subset(dataset, range(0,n_test))\n",
    "# dev_set = torch.utils.data.Subset(dataset, range(n_test, n))  # take the rest\n",
    "\n",
    "#split data into dev and test\n",
    "len_dev = int(len(dataset)*0.85)\n",
    "len_testtemp = len(dataset)-len_dev\n",
    "test_temp_set, dev_set = torch.utils.data.random_split(dataset, [len_testtemp, len_dev])\n",
    "\n",
    "dataset_duplicate_1 = DatasetFromSubset(dev_set, transform = train_transforms)\n",
    "dataset_duplicate_2 = DatasetFromSubset(dev_set, transform = train_transforms)\n",
    "dataset_duplicate_3 = DatasetFromSubset(dev_set, transform = train_transforms)\n",
    "\n",
    "#dataset_duplicate_1 = DatasetFromSubset(dev_set, transform = RandAugment_transforms)\n",
    "\n",
    "dev_set = dev_set + dataset_duplicate_1 + dataset_duplicate_2 + dataset_duplicate_3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "370c652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_loader = torch.utils.data.DataLoader(dataset = dev_set, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "#split dev into val and train\n",
    "train_len = int(len(dev_set)*0.85)\n",
    "val_len = len(dev_set)-train_len\n",
    "train_loader, val_loader = torch.utils.data.random_split(dev_set, [train_len, val_len])\n",
    "train_loader = torch.utils.data.DataLoader(train_loader, batch_size=64)\n",
    "val_loader = torch.utils.data.DataLoader(val_loader, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38929355",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f1d5948b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#resnet\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = models.wide_resnet50_2(pretrained=False)\n",
    "num_features = model.fc.in_features\n",
    "number_classes=3\n",
    "model.fc = nn.Linear(num_features, number_classes)\n",
    "device = set_device()\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#optimizer = optim = optim.Adam(resnet50_model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4e568ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alexnet\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f64f89d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "0.01\n",
      "training dataset: got 7852 out of 18842 images. Accuracy(41.673%), epoch loss 2.009\n",
      "testing dataset: got 1503 out of 3326 images. Accuracy(45.189%),\n",
      "epoch:  1\n",
      "0.01\n",
      "training dataset: got 8445 out of 18842 images. Accuracy(44.820%), epoch loss 1.130\n",
      "testing dataset: got 1554 out of 3326 images. Accuracy(46.723%),\n",
      "epoch:  2\n",
      "0.01\n",
      "training dataset: got 8767 out of 18842 images. Accuracy(46.529%), epoch loss 1.054\n",
      "testing dataset: got 1600 out of 3326 images. Accuracy(48.106%),\n",
      "epoch:  3\n",
      "0.01\n",
      "training dataset: got 9534 out of 18842 images. Accuracy(50.600%), epoch loss 1.007\n",
      "testing dataset: got 1787 out of 3326 images. Accuracy(53.728%),\n",
      "epoch:  4\n",
      "0.01\n",
      "training dataset: got 9977 out of 18842 images. Accuracy(52.951%), epoch loss 0.973\n",
      "testing dataset: got 1727 out of 3326 images. Accuracy(51.924%),\n",
      "epoch:  5\n",
      "0.01\n",
      "training dataset: got 10244 out of 18842 images. Accuracy(54.368%), epoch loss 0.949\n",
      "testing dataset: got 1878 out of 3326 images. Accuracy(56.464%),\n",
      "epoch:  6\n",
      "0.01\n",
      "training dataset: got 10599 out of 18842 images. Accuracy(56.252%), epoch loss 0.927\n",
      "testing dataset: got 1901 out of 3326 images. Accuracy(57.156%),\n",
      "epoch:  7\n",
      "0.01\n",
      "training dataset: got 10843 out of 18842 images. Accuracy(57.547%), epoch loss 0.910\n",
      "testing dataset: got 1882 out of 3326 images. Accuracy(56.584%),\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-03.\n",
      "epoch:  8\n",
      "0.005\n",
      "training dataset: got 11219 out of 18842 images. Accuracy(59.543%), epoch loss 0.876\n",
      "testing dataset: got 1997 out of 3326 images. Accuracy(60.042%),\n",
      "epoch:  9\n",
      "0.005\n",
      "training dataset: got 11269 out of 18842 images. Accuracy(59.808%), epoch loss 0.871\n",
      "testing dataset: got 2010 out of 3326 images. Accuracy(60.433%),\n",
      "epoch:  10\n",
      "0.005\n",
      "training dataset: got 11489 out of 18842 images. Accuracy(60.975%), epoch loss 0.854\n",
      "testing dataset: got 1996 out of 3326 images. Accuracy(60.012%),\n",
      "epoch:  11\n",
      "0.005\n",
      "training dataset: got 11483 out of 18842 images. Accuracy(60.944%), epoch loss 0.853\n",
      "testing dataset: got 2007 out of 3326 images. Accuracy(60.343%),\n",
      "epoch:  12\n",
      "0.005\n",
      "training dataset: got 11526 out of 18842 images. Accuracy(61.172%), epoch loss 0.845\n",
      "testing dataset: got 1906 out of 3326 images. Accuracy(57.306%),\n",
      "epoch:  13\n",
      "0.005\n",
      "training dataset: got 11622 out of 18842 images. Accuracy(61.681%), epoch loss 0.837\n",
      "testing dataset: got 2091 out of 3326 images. Accuracy(62.868%),\n",
      "epoch:  14\n",
      "0.005\n",
      "training dataset: got 11764 out of 18842 images. Accuracy(62.435%), epoch loss 0.827\n",
      "testing dataset: got 1997 out of 3326 images. Accuracy(60.042%),\n",
      "Epoch    15: reducing learning rate of group 0 to 2.5000e-03.\n",
      "epoch:  15\n",
      "0.0025\n",
      "training dataset: got 11958 out of 18842 images. Accuracy(63.465%), epoch loss 0.812\n",
      "testing dataset: got 2166 out of 3326 images. Accuracy(65.123%),\n",
      "epoch:  16\n",
      "0.0025\n",
      "training dataset: got 12048 out of 18842 images. Accuracy(63.942%), epoch loss 0.804\n",
      "testing dataset: got 2111 out of 3326 images. Accuracy(63.470%),\n",
      "epoch:  17\n",
      "0.0025\n",
      "training dataset: got 12053 out of 18842 images. Accuracy(63.969%), epoch loss 0.800\n",
      "testing dataset: got 2116 out of 3326 images. Accuracy(63.620%),\n",
      "epoch:  18\n",
      "0.0025\n",
      "training dataset: got 12137 out of 18842 images. Accuracy(64.415%), epoch loss 0.799\n",
      "testing dataset: got 2116 out of 3326 images. Accuracy(63.620%),\n",
      "epoch:  19\n",
      "0.0025\n",
      "training dataset: got 12181 out of 18842 images. Accuracy(64.648%), epoch loss 0.795\n",
      "testing dataset: got 2163 out of 3326 images. Accuracy(65.033%),\n",
      "epoch:  20\n",
      "0.0025\n",
      "training dataset: got 12271 out of 18842 images. Accuracy(65.126%), epoch loss 0.780\n",
      "testing dataset: got 2111 out of 3326 images. Accuracy(63.470%),\n",
      "epoch:  21\n",
      "0.0025\n",
      "training dataset: got 12292 out of 18842 images. Accuracy(65.237%), epoch loss 0.782\n",
      "testing dataset: got 2127 out of 3326 images. Accuracy(63.951%),\n",
      "Epoch    22: reducing learning rate of group 0 to 1.2500e-03.\n",
      "epoch:  22\n",
      "0.00125\n",
      "training dataset: got 12560 out of 18842 images. Accuracy(66.660%), epoch loss 0.758\n",
      "testing dataset: got 2188 out of 3326 images. Accuracy(65.785%),\n",
      "epoch:  23\n",
      "0.00125\n",
      "training dataset: got 12671 out of 18842 images. Accuracy(67.249%), epoch loss 0.753\n",
      "testing dataset: got 2223 out of 3326 images. Accuracy(66.837%),\n",
      "epoch:  24\n",
      "0.00125\n",
      "training dataset: got 12657 out of 18842 images. Accuracy(67.174%), epoch loss 0.751\n",
      "testing dataset: got 2206 out of 3326 images. Accuracy(66.326%),\n",
      "epoch:  25\n",
      "0.00125\n",
      "training dataset: got 12785 out of 18842 images. Accuracy(67.854%), epoch loss 0.742\n",
      "testing dataset: got 2217 out of 3326 images. Accuracy(66.657%),\n",
      "epoch:  26\n",
      "0.00125\n",
      "training dataset: got 12708 out of 18842 images. Accuracy(67.445%), epoch loss 0.740\n",
      "testing dataset: got 2208 out of 3326 images. Accuracy(66.386%),\n",
      "epoch:  27\n",
      "0.00125\n",
      "training dataset: got 12826 out of 18842 images. Accuracy(68.071%), epoch loss 0.735\n",
      "testing dataset: got 2172 out of 3326 images. Accuracy(65.304%),\n",
      "epoch:  28\n",
      "0.00125\n",
      "training dataset: got 12779 out of 18842 images. Accuracy(67.822%), epoch loss 0.739\n",
      "testing dataset: got 2227 out of 3326 images. Accuracy(66.957%),\n",
      "Epoch    29: reducing learning rate of group 0 to 6.2500e-04.\n",
      "epoch:  29\n",
      "0.000625\n",
      "training dataset: got 12995 out of 18842 images. Accuracy(68.968%), epoch loss 0.718\n",
      "testing dataset: got 2275 out of 3326 images. Accuracy(68.400%),\n",
      "epoch:  30\n",
      "0.000625\n",
      "training dataset: got 12999 out of 18842 images. Accuracy(68.989%), epoch loss 0.715\n",
      "testing dataset: got 2275 out of 3326 images. Accuracy(68.400%),\n",
      "epoch:  31\n",
      "0.000625\n",
      "training dataset: got 13081 out of 18842 images. Accuracy(69.425%), epoch loss 0.707\n",
      "testing dataset: got 2300 out of 3326 images. Accuracy(69.152%),\n",
      "epoch:  32\n",
      "0.000625\n",
      "training dataset: got 13202 out of 18842 images. Accuracy(70.067%), epoch loss 0.701\n",
      "testing dataset: got 2260 out of 3326 images. Accuracy(67.949%),\n",
      "epoch:  33\n",
      "0.000625\n",
      "training dataset: got 13145 out of 18842 images. Accuracy(69.764%), epoch loss 0.697\n",
      "testing dataset: got 2264 out of 3326 images. Accuracy(68.070%),\n",
      "epoch:  34\n",
      "0.000625\n",
      "training dataset: got 13165 out of 18842 images. Accuracy(69.871%), epoch loss 0.697\n",
      "testing dataset: got 2301 out of 3326 images. Accuracy(69.182%),\n",
      "epoch:  35\n",
      "0.000625\n",
      "training dataset: got 13130 out of 18842 images. Accuracy(69.685%), epoch loss 0.693\n",
      "testing dataset: got 2246 out of 3326 images. Accuracy(67.529%),\n",
      "Epoch    36: reducing learning rate of group 0 to 3.1250e-04.\n",
      "epoch:  36\n",
      "0.0003125\n",
      "training dataset: got 13354 out of 18842 images. Accuracy(70.874%), epoch loss 0.677\n",
      "testing dataset: got 2307 out of 3326 images. Accuracy(69.363%),\n",
      "epoch:  37\n",
      "0.0003125\n",
      "training dataset: got 13387 out of 18842 images. Accuracy(71.049%), epoch loss 0.668\n",
      "testing dataset: got 2285 out of 3326 images. Accuracy(68.701%),\n",
      "epoch:  38\n",
      "0.0003125\n",
      "training dataset: got 13482 out of 18842 images. Accuracy(71.553%), epoch loss 0.665\n",
      "testing dataset: got 2314 out of 3326 images. Accuracy(69.573%),\n",
      "epoch:  39\n",
      "0.0003125\n",
      "training dataset: got 13405 out of 18842 images. Accuracy(71.144%), epoch loss 0.669\n",
      "testing dataset: got 2305 out of 3326 images. Accuracy(69.302%),\n",
      "epoch:  40\n",
      "0.0003125\n",
      "training dataset: got 13466 out of 18842 images. Accuracy(71.468%), epoch loss 0.663\n",
      "testing dataset: got 2299 out of 3326 images. Accuracy(69.122%),\n",
      "epoch:  41\n",
      "0.0003125\n",
      "training dataset: got 13516 out of 18842 images. Accuracy(71.733%), epoch loss 0.660\n",
      "testing dataset: got 2307 out of 3326 images. Accuracy(69.363%),\n",
      "epoch:  42\n",
      "0.0003125\n",
      "training dataset: got 13615 out of 18842 images. Accuracy(72.259%), epoch loss 0.653\n",
      "testing dataset: got 2331 out of 3326 images. Accuracy(70.084%),\n",
      "Epoch    43: reducing learning rate of group 0 to 1.5625e-04.\n",
      "epoch:  43\n",
      "0.00015625\n",
      "training dataset: got 13706 out of 18842 images. Accuracy(72.742%), epoch loss 0.645\n",
      "testing dataset: got 2290 out of 3326 images. Accuracy(68.851%),\n",
      "epoch:  44\n",
      "0.00015625\n",
      "training dataset: got 13728 out of 18842 images. Accuracy(72.859%), epoch loss 0.643\n",
      "testing dataset: got 2322 out of 3326 images. Accuracy(69.814%),\n",
      "epoch:  45\n",
      "0.00015625\n",
      "training dataset: got 13803 out of 18842 images. Accuracy(73.257%), epoch loss 0.636\n",
      "testing dataset: got 2296 out of 3326 images. Accuracy(69.032%),\n",
      "epoch:  46\n",
      "0.00015625\n",
      "training dataset: got 13724 out of 18842 images. Accuracy(72.837%), epoch loss 0.634\n",
      "testing dataset: got 2288 out of 3326 images. Accuracy(68.791%),\n",
      "epoch:  47\n",
      "0.00015625\n",
      "training dataset: got 13752 out of 18842 images. Accuracy(72.986%), epoch loss 0.640\n",
      "testing dataset: got 2312 out of 3326 images. Accuracy(69.513%),\n",
      "epoch:  48\n",
      "0.00015625\n",
      "training dataset: got 13816 out of 18842 images. Accuracy(73.326%), epoch loss 0.634\n",
      "testing dataset: got 2318 out of 3326 images. Accuracy(69.693%),\n",
      "epoch:  49\n",
      "0.00015625\n",
      "training dataset: got 13722 out of 18842 images. Accuracy(72.827%), epoch loss 0.630\n",
      "testing dataset: got 2317 out of 3326 images. Accuracy(69.663%),\n",
      "Epoch    50: reducing learning rate of group 0 to 7.8125e-05.\n",
      "epoch:  50\n",
      "7.8125e-05\n",
      "training dataset: got 13908 out of 18842 images. Accuracy(73.814%), epoch loss 0.627\n",
      "testing dataset: got 2321 out of 3326 images. Accuracy(69.784%),\n",
      "epoch:  51\n",
      "7.8125e-05\n",
      "training dataset: got 13954 out of 18842 images. Accuracy(74.058%), epoch loss 0.621\n",
      "testing dataset: got 2338 out of 3326 images. Accuracy(70.295%),\n",
      "epoch:  52\n",
      "7.8125e-05\n",
      "training dataset: got 13943 out of 18842 images. Accuracy(74.000%), epoch loss 0.615\n",
      "testing dataset: got 2306 out of 3326 images. Accuracy(69.333%),\n",
      "epoch:  53\n",
      "7.8125e-05\n",
      "training dataset: got 13970 out of 18842 images. Accuracy(74.143%), epoch loss 0.617\n",
      "testing dataset: got 2286 out of 3326 images. Accuracy(68.731%),\n",
      "overfitting.(the updated validation accuracy is less than the minimum validation accuracies of last 10 epochs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use learning rate scheduler to train the model\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "def evaluate_model_on_test_set(model, test_loader):\n",
    "    model.eval()\n",
    "    predicted_correctly_on_epoch = 0\n",
    "    total = 0\n",
    "    device = set_device()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            total+= labels.size(0)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            predicted_correctly_on_epoch += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_acc = 100*predicted_correctly_on_epoch/total\n",
    "    print('testing dataset: got %d out of %d images. Accuracy(%.3f%%),'\n",
    "             %(predicted_correctly_on_epoch, total, epoch_acc))\n",
    "    return epoch_acc\n",
    "\n",
    "# Define the learning rate schedule function\n",
    "def lr_schedule(epoch):\n",
    "#     if epoch < 10:\n",
    "#         return 0.02\n",
    "#     elif epoch < 20:\n",
    "#         return 0.01\n",
    "#     else:\n",
    "#         return 0.001\n",
    "    return 0.01\n",
    "\n",
    "def train_nn(model, train_loader, test_loader, criterion, epochs):\n",
    "    validation_accuracy_array = [0,0,0,0,0,0,0,0,0,0]\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor = 0.5, patience=6,verbose=True)\n",
    "    device = set_device()\n",
    "    for epoch in range(epochs):        \n",
    "        print('epoch: ',epoch)\n",
    "        model.train()\n",
    "        running_loss=0\n",
    "        running_correct=0\n",
    "        total=0\n",
    "        for data in train_loader:\n",
    "\n",
    "            images, labels = data\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            total+= labels.size(0)\n",
    "            #plt.imshow(images)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "#             print(outputs.data.shape)\n",
    "#             print(predicted)\n",
    "#             print()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            running_correct += (labels==predicted).sum().item()\n",
    "        print(optimizer.param_groups[0]['lr'])\n",
    "        epoch_loss = running_loss/len(train_loader)\n",
    "        epoch_accuracy = 100*running_correct/total\n",
    "        print('training dataset: got %d out of %d images. Accuracy(%.3f%%), epoch loss %.3f'\n",
    "             %(running_correct, total, epoch_accuracy, epoch_loss))\n",
    "        validation_accuracy = evaluate_model_on_test_set(model, test_loader)\n",
    "        scheduler.step(validation_accuracy)\n",
    "        if validation_accuracy > min(validation_accuracy_array):\n",
    "            validation_accuracy_array[epoch%10] = validation_accuracy\n",
    "        else:\n",
    "            print(\"overfitting.(the updated validation accuracy is less than the minimum validation accuracies of last 10 epochs)\")\n",
    "            break\n",
    "    return model\n",
    "\n",
    "\n",
    "train_nn(model, train_loader, val_loader, loss_function, 150)\n",
    "# add early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0e0a4613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7720, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_temp_loader = torch.utils.data.DataLoader(dataset = test_temp_set, batch_size=64, shuffle=True)\n",
    "model.eval()\n",
    "device = set_device()\n",
    "# Use the model to predict the outputs for the test data\n",
    "predicted_outputs = []\n",
    "true_outputs = []\n",
    "for input_data, target_data in test_temp_loader:\n",
    "    input_data = input_data.to(device)\n",
    "    target_data = target_data.to(device)\n",
    "    # Feed the input data into the model and compute the predicted outputs\n",
    "    output = model(input_data)\n",
    "    _, predicted = torch.max(output.data,1)\n",
    "    # Store the predicted and true outputs\n",
    "    predicted_outputs.append(predicted)\n",
    "    true_outputs.append(target_data)\n",
    "predicted_outputs = torch.cat(predicted_outputs)\n",
    "true_outputs = torch.cat(true_outputs)\n",
    "accuracy = torch.mean((predicted_outputs == true_outputs).to(torch.float))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "89733f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2297, 0.2273, 0.2195]), tensor([0.6136, 0.6157, 0.6193]))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a2921d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file\n",
    "with open('super_wrs_1.pkl', 'wb') as f:\n",
    "  pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e28a5aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5650666666666666"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "resnet18 = [0.7642,0.7693, 0.7580]\n",
    "resnet18 = statistics.mean(resnet18)\n",
    "resnet34 = [0.7302,0.6869, 0.7683]\n",
    "resnet34 = statistics.mean(resnet34)\n",
    "resnet50 = [0.7065,0.5345, 0.4573]\n",
    "resnet50 = statistics.mean(resnet50)\n",
    "resnet101 = [0.3996,0.4861, 0.6622]\n",
    "resnet101 = statistics.mean(resnet101)\n",
    "resnet152 = [0.5942, 0.6262,0.4748]\n",
    "resnet152 = statistics.mean(resnet152)\n",
    "\n",
    "wideresnet50 = \n",
    "#models = ['ResNet18','ResNet34','ResNet50', 'ResNet101','ResNet152']\n",
    "resnet152"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb72fd0",
   "metadata": {},
   "source": [
    "resnet50:\n",
    "lr = 0.01 epoch 20 test accuracy 70.44% \n",
    "dynamic lr 10/20 0.02/0.01/0.001 epoch 30 test accuracy 75\n",
    "lr = 0.01 epoch 30 test accuracy 77.86% \n",
    "\n",
    "resnet 152:\n",
    " test accuracy 68.59%\n",
    "resnext101_32x8d\n",
    "dynamic lr 10/20 0.02/0.01/0.001 epoch 30 test accuracy 75.39%\n",
    "classify subcategory: \n",
    "lr 0.01 epoch 20 without doing contrast test accuracy 9.37%\n",
    "lr 0.01 epoch 20 with contrast test accuracy 12.87%\n",
    "lr 0.01 epoch 200 with contrast, test accuracy 21.06%/10.6%\n",
    "\n",
    "wide_resnet50_2:\n",
    "30 epochs 71.88%\n",
    "0.74.56% with auto contrast\n",
    "0.7353 with auto contrast and RandomAdjustSharpness\n",
    "77.96 epoch 62\n",
    "\n",
    "wide_resnet101_2\n",
    "lr 0.01 epoch 100, test accuracy 22.19%/14.1%\n",
    "lr 0.01 epoch 100 with ReduceLROnPlateau accuracy 21.987/13.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_super_class_pred(predicted_outputs):\n",
    "#     super_class = []\n",
    "#     for i in predicted_outputs:\n",
    "#         if i.item() in sub_class0:\n",
    "#             super_class.append(0)\n",
    "#         elif i.item() in sub_class1:\n",
    "#             super_class.append(1)\n",
    "#         elif i.item() in sub_class2:\n",
    "#             super_class.append(2)\n",
    "#     return super_class\n",
    "\n",
    "# def get_similarity(arr1, arr2):\n",
    "#     # Calculate the number of elements that are the same in both arrays\n",
    "#     same_count = sum(1 for a, b in zip(arr1, arr2) if a == b)\n",
    "\n",
    "#     # Calculate the total number of elements in both arrays\n",
    "#     total_count = len(arr1) + len(arr2)\n",
    "\n",
    "#     # Calculate the similarity percentage\n",
    "#     similarity = same_count / total_count\n",
    "\n",
    "#     return similarity\n",
    "# # Evaluate the model using test-temp data\n",
    "# test_temp_loader = torch.utils.data.DataLoader(dataset = test_temp_set, batch_size=64, shuffle=True)\n",
    "# model.eval()\n",
    "# device = set_device()\n",
    "# # Use the model to predict the outputs for the test data\n",
    "# predicted_sub_outputs = []\n",
    "# true_sub_outputs = []\n",
    "# for input_data, target_data in test_temp_loader:\n",
    "#     input_data = input_data.to(device)\n",
    "#     target_data = target_data.to(device)\n",
    "#     # Feed the input data into the model and compute the predicted outputs\n",
    "#     output = model(input_data)\n",
    "#     _, predicted = torch.max(output.data,1)\n",
    "#     # Store the predicted and true outputs\n",
    "#     predicted_sub_outputs.append(predicted)\n",
    "#     true_sub_outputs.append(target_data)\n",
    "\n",
    "# predicted_sub_outputs = torch.cat(predicted_sub_outputs)\n",
    "# true_sub_outputs = torch.cat(true_sub_outputs)\n",
    "# true_super_outputs = get_super_class_pred(true_sub_outputs)\n",
    "# sub_accuracy = torch.mean((predicted_sub_outputs == true_sub_outputs).to(torch.float))\n",
    "\n",
    "\n",
    "# predicted_super_outputs = get_super_class_pred(predicted_sub_outputs)\n",
    "\n",
    "# super_accuracy = get_similarity(predicted_super_outputs,true_super_outputs)*100\n",
    "\n",
    "\n",
    "# print('super class accuracy', super_accuracy)\n",
    "# print(\"sub class accuracy\",sub_accuracy.item())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
